{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gordeli/NLP_EDHEC2023/blob/main/colab/02_Data_Collection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "APfI_c8B40Vn"
   },
   "source": [
    "#Natural Language Processing @ EDHEC\n",
    "\n",
    "# Part 2: Data Collection\n",
    "\n",
    "[<- Previous: Text Processing Basics](https://colab.research.google.com/github/gordeli/NLP_EDHEC2024/blob/main/colab/01_Text_Processing_Basics.ipynb)\n",
    "\n",
    "[-> Next: Corpus Level Processing](https://colab.research.google.com/github/gordeli/NLP_EDHEC2024/blob/main/colab/03_Corpus_Level_Processing.ipynb)\n",
    "\n",
    "\n",
    "Facilitator: [Ivan Gordeliy](https://www.linkedin.com/in/gordeli/)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdTajgZhkGWX"
   },
   "source": [
    "## Initial Setup\n",
    "\n",
    "- **Run \"Setup\" below first.**\n",
    "\n",
    "    - This will load libraries and download some resources that we'll use throughout the tutorial.\n",
    "\n",
    "    - You will see a message reading \"Done with setup!\" when this process completes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GKVEnPi34qj4"
   },
   "outputs": [],
   "source": [
    "#@title Setup (click the \"run\" button to the left) {display-mode: \"form\"}\n",
    "\n",
    "## Setup ##\n",
    "\n",
    "# imports\n",
    "\n",
    "# built-in Python libraries\n",
    "# -------------------------\n",
    "\n",
    "# For processing the incoming Twitter data\n",
    "import json\n",
    "# os-level utils\n",
    "import os\n",
    "# For downloading web data\n",
    "import requests\n",
    "# For compressing files\n",
    "import zipfile\n",
    "\n",
    "# 3rd party libraries\n",
    "# -------------------\n",
    "\n",
    "# beautiful soup for html parsing\n",
    "!pip install beautifulsoup4\n",
    "import bs4\n",
    "\n",
    "# tweepy for using the Twitter API\n",
    "# !pip install tweepy\n",
    "# import tweepy\n",
    "\n",
    "# snscrape for scraping Twitter\n",
    "# !pip3 install snscrape\n",
    "\n",
    "\n",
    "# allows downloading of files from colab to your computer\n",
    "from google.colab import files\n",
    "\n",
    "# get sample reddit data\n",
    "if not os.path.exists(\"reddit_2019_05_5K.json\"):\n",
    "    !wget https://raw.githubusercontent.com/gordeli/textanalysis/master/data/reddit_2019_05_5K.json\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Done with setup!\")\n",
    "print(\"If you'd like, you can click the (X) button to the left to clear this output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cyPPpRkI6i0n"
   },
   "source": [
    "---\n",
    "\n",
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hMYV2R0h6DTN"
   },
   "source": [
    "- Here we'll cover a few different sources of user-generated content and provide some examples of how to gather data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZhKTVFO6q2b"
   },
   "source": [
    "### Web Scraping and HTML parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EeBSAsuOHJ3Z"
   },
   "source": [
    "- Lots of text data is available directly from web pages.\n",
    "- Have a look at the following website: [Quotes to Scrape](http://quotes.toscrape.com/page/1/)\n",
    "- With the Beautiful Soup library, it's very easy to take some html and extract only the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TA4bHC-06uXb"
   },
   "outputs": [],
   "source": [
    "html_content = requests.get(\"http://quotes.toscrape.com/page/1/\").content\n",
    "soup = bs4.BeautifulSoup(html_content,\"html.parser\")\n",
    "print(soup.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXKZcbEIHjOW"
   },
   "source": [
    "- If you want to extract data in a more targeted way, you can navitage the [html document object model](https://www.w3schools.com/whatis/whatis_htmldom.asp) using [Beautiful Soup functions](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), but we won't dive deeply into this for now,\n",
    "- **Important: You should not use this kind of code to just go collect data from any website!**\n",
    "    - Web scaping tools should always check a site's [`robots.txt` file](https://www.robotstxt.org/robotstxt.html), which describes how crawlers, scrapers, indexers, etc., should use the site.\n",
    "        - For example, see [github's robots.txt](https://github.com/robots.txt)\n",
    "    - You should be able to find any site's robots.txt (if there is one) at http://\\<domain\\>/robots.txt for any web \\<domain\\>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Be60X7Eu6zd2"
   },
   "source": [
    "### Reddit Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAWPIB1E_rU9"
   },
   "source": [
    "- Reddit is a great source of publicly available user-generated content.\n",
    "- We could scrape Reddit ourselves, but why do that if someone has already (generously) done the heavy lifting?\n",
    "- Most Reddit is available for researchers to download.\n",
    "- [Updated in 2003](https://www.reddit.com/r/pushshift/)\n",
    "- [Alternative hosted by Cornell up to Oct 2018](https://convokit.cornell.edu/documentation/subreddit.html)\n",
    "- [Alternative on Github up to Mar 2023](https://github.com/ArthurHeitmann/arctic_shift)\n",
    "- Let's explore a small subset of the data from May 2019:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3KftObP6_Rb"
   },
   "outputs": [],
   "source": [
    "# read the data that was downloaded during setup\n",
    "# this is the exact format as the full corpus, just truncated to the first 5000 lines\n",
    "sample_reddit_posts_raw = open(\"reddit_2019_05_5K.json\",'r').readlines()\n",
    "print(\"Loaded\",len(sample_reddit_posts_raw),\"reddit posts.\")\n",
    "reddit_json = [json.loads(post) for post in sample_reddit_posts_raw]\n",
    "print(json.dumps(reddit_json[50], sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oXB5iK87hLtg"
   },
   "source": [
    "- Since the posts are in json format, we used the Python json library to process them.\n",
    "    - This library returns Python dict objects, so we can access them just like we would any other dictionary.\n",
    "- Let's view some of the text content from these posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTQbFr4zdsh_"
   },
   "outputs": [],
   "source": [
    "for post in reddit_json[:100]:\n",
    "    if post['selftext'].strip() and post['selftext'] not in [\"[removed]\",\"[deleted]\"]:\n",
    "        print(\"Subreddit:\",post['subreddit'],\"\\nTitle:\",post['title'],\"\\nContent:\", \\\n",
    "              post['selftext'],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-dIHIU2gs3K"
   },
   "source": [
    "- Note that we filtered out posts with no text content.\n",
    "    - Many posts have a non-null \"media\" field, which could contain images, links to youtube, videos, etc.\n",
    "        - These could be worth exploring more, using computer vision to process images/videos and NLP to process linked websites.\n",
    "- That covers the basics of getting Reddit data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etUZgi-twrES"
   },
   "source": [
    "- [-> Next: Corpus Level Processing](https://colab.research.google.com/github/gordeli/NLP_EDHEC2024/blob/main/colab/03_Corpus_Level_Processing.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "02_Data_Collection.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
