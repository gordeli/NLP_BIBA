{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gordeli/BIfTA/blob/main/colab/02_Data_Collection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "APfI_c8B40Vn"
   },
   "source": [
    "#Bevavior Insights from Text Analysis @ EDHEC, 2023\n",
    "\n",
    "# Part 2: Data Collection\n",
    "\n",
    "[<- Previous: Text Processing Basics](https://colab.research.google.com/github/gordeli/BIfTA/blob/main/colab/01_Text_Processing_Basics.ipynb)\n",
    "\n",
    "[-> Next: Corpus Level Processing](https://colab.research.google.com/github/gordeli/BIfTA/blob/main/colab/03_Corpus_Level_Processing.ipynb)\n",
    "\n",
    "Dates: January 11-27, 2023\n",
    "\n",
    "Facilitator: [Ivan Gordeliy](https://www.linkedin.com/in/gordeli/)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdTajgZhkGWX"
   },
   "source": [
    "## Initial Setup\n",
    "\n",
    "- **Run \"Setup\" below first.**\n",
    "\n",
    "    - This will load libraries and download some resources that we'll use throughout the tutorial.\n",
    "\n",
    "    - You will see a message reading \"Done with setup!\" when this process completes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GKVEnPi34qj4"
   },
   "outputs": [],
   "source": [
    "#@title Setup (click the \"run\" button to the left) {display-mode: \"form\"}\n",
    "\n",
    "## Setup ##\n",
    "\n",
    "# imports\n",
    "\n",
    "# built-in Python libraries\n",
    "# -------------------------\n",
    "\n",
    "# For processing the incoming Twitter data\n",
    "import json\n",
    "# os-level utils\n",
    "import os\n",
    "# For downloading web data\n",
    "import requests\n",
    "# For compressing files\n",
    "import zipfile\n",
    "\n",
    "# 3rd party libraries\n",
    "# -------------------\n",
    "\n",
    "# beautiful soup for html parsing\n",
    "!pip install beautifulsoup4\n",
    "import bs4\n",
    "\n",
    "# tweepy for using the Twitter API\n",
    "# !pip install tweepy\n",
    "# import tweepy\n",
    "\n",
    "# snscrape for scraping Twitter\n",
    "!pip3 install snscrape\n",
    "\n",
    "\n",
    "# allows downloading of files from colab to your computer\n",
    "from google.colab import files\n",
    "\n",
    "# get sample reddit data\n",
    "if not os.path.exists(\"reddit_2019_05_5K.json\"):\n",
    "    !wget https://raw.githubusercontent.com/gordeli/textanalysis/master/data/reddit_2019_05_5K.json\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Done with setup!\")\n",
    "print(\"If you'd like, you can click the (X) button to the left to clear this output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cyPPpRkI6i0n"
   },
   "source": [
    "---\n",
    "\n",
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hMYV2R0h6DTN"
   },
   "source": [
    "- Here we'll cover a few different sources of user-generated content and provide some examples of how to gather data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZhKTVFO6q2b"
   },
   "source": [
    "### Web Scraping and HTML parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EeBSAsuOHJ3Z"
   },
   "source": [
    "- Lots of text data is available directly from web pages.\n",
    "- Have a look at the following website: [Quotes to Scrape](http://quotes.toscrape.com/page/1/)\n",
    "- With the Beautiful Soup library, it's very easy to take some html and extract only the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TA4bHC-06uXb"
   },
   "outputs": [],
   "source": [
    "html_content = requests.get(\"http://quotes.toscrape.com/page/1/\").content\n",
    "soup = bs4.BeautifulSoup(html_content,\"html.parser\")\n",
    "print(soup.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXKZcbEIHjOW"
   },
   "source": [
    "- If you want to extract data in a more targeted way, you can navitage the [html document object model](https://www.w3schools.com/whatis/whatis_htmldom.asp) using [Beautiful Soup functions](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), but we won't dive deeply into this for now,\n",
    "- **Important: You should not use this kind of code to just go collect data from any website!**\n",
    "    - Web scaping tools should always check a site's [`robots.txt` file](https://www.robotstxt.org/robotstxt.html), which describes how crawlers, scrapers, indexers, etc., should use the site.\n",
    "        - For example, see [github's robots.txt](https://github.com/robots.txt)\n",
    "    - You should be able to find any site's robots.txt (if there is one) at http://\\<domain\\>/robots.txt for any web \\<domain\\>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Be60X7Eu6zd2"
   },
   "source": [
    "### Reddit Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAWPIB1E_rU9"
   },
   "source": [
    "- Reddit is a great source of publicly available user-generated content.\n",
    "- We could scrape Reddit ourselves, but why do that if someone has already (generously) done the heavy lifting?\n",
    "- Reddit user Stuck_in_the_Matrix has compiled and compressed essentially all of Reddit for researchers to download.\n",
    "- [Original submissions corpus](https://www.reddit.com/r/datasets/comments/3mg812/full_reddit_submission_corpus_now_available_2006/) (up to 2015) and [updates](https://files.pushshift.io/reddit/submissions/) (up to January 2023 at the time of latest update of this notebook).\n",
    "    - For a smaller file to get started with, take a look at the [daily comments files](https://files.pushshift.io/reddit/comments/daily/).\n",
    "    - To explore more files available, see [this top-level directory](https://files.pushshift.io/reddit/).\n",
    "- Let's explore a small subset of the data from May 2019:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3KftObP6_Rb"
   },
   "outputs": [],
   "source": [
    "# read the data that was downloaded during setup\n",
    "# this is the exact format as the full corpus, just truncated to the first 5000 lines\n",
    "sample_reddit_posts_raw = open(\"reddit_2019_05_5K.json\",'r').readlines()\n",
    "print(\"Loaded\",len(sample_reddit_posts_raw),\"reddit posts.\")\n",
    "reddit_json = [json.loads(post) for post in sample_reddit_posts_raw]\n",
    "print(json.dumps(reddit_json[50], sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oXB5iK87hLtg"
   },
   "source": [
    "- Since the posts are in json format, we used the Python json library to process them.\n",
    "    - This library returns Python dict objects, so we can access them just like we would any other dictionary.\n",
    "- Let's view some of the text content from these posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTQbFr4zdsh_"
   },
   "outputs": [],
   "source": [
    "for post in reddit_json[:100]:\n",
    "    if post['selftext'].strip() and post['selftext'] not in [\"[removed]\",\"[deleted]\"]:\n",
    "        print(\"Subreddit:\",post['subreddit'],\"\\nTitle:\",post['title'],\"\\nContent:\", \\\n",
    "              post['selftext'],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-dIHIU2gs3K"
   },
   "source": [
    "- Note that we filtered out posts with no text content.\n",
    "    - Many posts have a non-null \"media\" field, which could contain images, links to youtube, videos, etc.\n",
    "        - These could be worth exploring more, using computer vision to process images/videos and NLP to process linked websites.\n",
    "- That covers the basics of getting Reddit data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uxyARJY46vKO"
   },
   "source": [
    "### snscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zkPEhJn_Kga0"
   },
   "source": [
    "- Twitter is also known for being an abundant source of publc text data (perhaps even more so than Reddit).\n",
    "- For this tutorial, we'll look at using the [snscrape scraper](https://github.com/JustAnotherArchivist/snscrape), which allows us to retreive tweets that contain specific words, phrases, and hashtags.\n",
    "- In the slides, we talked about how to setup a Twitter App and get a API keys.\n",
    "    - You should add your own keys below and then run the code block to set your keys:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing snscrape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install snscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running snscrape from command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snscrape --jsonl --progress --max-results 100 --since 2019-01-01 twitter-search \"macbook filter:verified lang:en until:2019-01-02\" > tweets.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# A simple python code to scrape Twitter using snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('snscrape --jsonl --progress --max-results 100 --since 2019-01-01 twitter-search \"macbook filter:verified lang:en until:2019-01-02\" > tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip the next 2 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "years = ['2019', '2020']\n",
    "months =['04', '05']\n",
    "keywords = ['macbook', 'surface']\n",
    "\n",
    "# cmd = \"snscrape --jsonl --progress --since \" + year + '-' + month + '01 twitter-search \\\"ad filter:verified lang:en until:' year + '-' + month_next + '-02\\\" \\> tweets' + year + month + '_filterverified_' + k + '_en.json'\n",
    "for k in keywords:\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            month_next = '0' + str((int(month) % 12 + 1))\n",
    "            month_next = month_next[-2:]\n",
    "            cmd = 'snscrape --jsonl --progress --since ' + year + '-' + month + '-01' + ' twitter-search \\\"' + k + ' filter:verified lang:en until:' + year + '-' + month_next + '-01' '\\\" > tweets' + year + month + '_filterverified_' + k + '_en.json'\n",
    "            os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import datetime # to handle dates convertions and interpretation\n",
    "import time\n",
    "\n",
    "years = ['2019', '2020']\n",
    "months =['04', '05']\n",
    "keywords = ['macbook', 'surface']\n",
    "\n",
    "fnames = []\n",
    "\n",
    "for k in keywords:\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            f = 'tweets' + year + month + '_filterverified_' + k + '_en.json'\n",
    "            fnames.append(f)\n",
    "\n",
    "conn = sqlite3.connect('tweets.sqlite')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import datetime # to handle dates convertions and interpretation\n",
    "import time\n",
    "\n",
    "fnames = ['tweets.json']\n",
    "\n",
    "conn = sqlite3.connect('tweets.sqlite')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.executescript('''\n",
    "DROP TABLE IF EXISTS Tweets;\n",
    "DROP TABLE IF EXISTS Users;\n",
    "DROP TABLE IF EXISTS Labels;\n",
    "DROP TABLE IF EXISTS Places;\n",
    "DROP TABLE IF EXISTS Sources;\n",
    "DROP TABLE IF EXISTS Hashtags;\n",
    "DROP TABLE IF EXISTS Cashtags;\n",
    "\n",
    "\n",
    "CREATE TABLE Tweets (\n",
    "    id                 INTEGER NOT NULL PRIMARY KEY UNIQUE,\n",
    "    date               INTEGER NOT NULL,\n",
    "    content            TEXT,\n",
    "    renderedContent    TEXT,\n",
    "    replyCount         INTEGER NOT NULL,\n",
    "    retweetCount       INTEGER NOT NULL,\n",
    "    likeCount          INTEGER NOT NULL,\n",
    "    quoteCount         INTEGER NOT NULL,\n",
    "    lang               TEXT NOT NULL,\n",
    "    url                TEXT NOT NULL,\n",
    "    coordinates        TEXT,\n",
    "    hashtags           TEXT,\n",
    "    cashtags           TEXT,\n",
    "    user_id            INTEGER NOT NULL,\n",
    "    mentionedUsers_ids TEXT,\n",
    "    inReplyToTweetId   INTEGER,\n",
    "    quotedTweetId      INTEGER,\n",
    "    inReplyToUser_id   INTEGER,\n",
    "    conversationId     INTEGER NOT NULL,\n",
    "    source_id          INTEGER NOT NULL,\n",
    "    place_id           INTEGER\n",
    "\n",
    ");\n",
    "\n",
    "CREATE TABLE Users (\n",
    "    id               INTEGER NOT NULL PRIMARY KEY UNIQUE,\n",
    "    username         TEXT NOT NULL UNIQUE,\n",
    "    displayname      TEXT NOT NULL,\n",
    "    description      TEXT,\n",
    "    rawDescription   TEXT,\n",
    "    descriptionUrls  TEXT,\n",
    "    verified         INTEGER,\n",
    "    created          INTEGER NOT NULL,\n",
    "    followersCount   INTEGER NOT NULL,\n",
    "    following        INTEGER NOT NULL,\n",
    "    statusesCount    INTEGER NOT NULL,\n",
    "    favouritesCount  INTEGER NOT NULL,\n",
    "    listedCount      INTEGER NOT NULL,\n",
    "    mediaCount       INTEGER,\n",
    "    location         TEXT,\n",
    "    protected        INTEGER,\n",
    "    linkUrl          TEXT,\n",
    "    profileImageUrl  TEXT,\n",
    "    profileBannerUrl TEXT,\n",
    "    url              TEXT,\n",
    "    label_id         INTEGER\n",
    ");\n",
    "\n",
    "CREATE TABLE Labels (\n",
    "    id              INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    description     TEXT NOT NULL UNIQUE,\n",
    "    url             TEXT,\n",
    "    badgeUrl        TEXT,\n",
    "    longDescription TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE Places (\n",
    "    id            INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    fullName      TEXT NOT NULL,\n",
    "    name          TEXT,\n",
    "    type          TEXT,\n",
    "    country       TEXT,\n",
    "    countryCode   TEXT,\n",
    "    UNIQUE(fullName, type)\n",
    ");\n",
    "\n",
    "CREATE TABLE Sources (\n",
    "    id            INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    sourceLabel   TEXT NOT NULL UNIQUE,\n",
    "    source        TEXT,\n",
    "    sourceUrl     TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE Hashtags (\n",
    "    id           INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    hashtag      TEXT NOT NULL UNIQUE\n",
    ");\n",
    "\n",
    "CREATE TABLE Cashtags (\n",
    "    id           INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    cashtag      TEXT NOT NULL UNIQUE\n",
    ")\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false = False\n",
    "true = True\n",
    "null = None\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for fname in fnames:\n",
    "    data = [json.loads(line) for line in open(fname, 'r')]\n",
    "    for i in data:\n",
    "        # dic = eval(l)\n",
    "\n",
    "        ## Labels data####\n",
    "        if i['user']['label'] == None:\n",
    "            label_id = None\n",
    "        else:\n",
    "            label_description = i['user']['label']['description']\n",
    "            label_url = i['user']['label']['url']\n",
    "            badgeUrl = i['user']['label']['badgeUrl']\n",
    "            longDescription = i['user']['label']['longDescription']\n",
    "            cur.execute('''INSERT OR IGNORE INTO Labels (description, url, badgeUrl, longDescription)\n",
    "            VALUES ( ?, ? , ? , ?  )''', ( label_description, label_url, badgeUrl, longDescription) )\n",
    "            cur.execute('SELECT id FROM Labels WHERE description = ? ', (label_description, ))\n",
    "            label_id = cur.fetchone()[0]\n",
    "\n",
    "        ## Places data####\n",
    "        if i['place'] == None:\n",
    "            place_id = None\n",
    "        else:\n",
    "            fullName = i['place']['fullName']\n",
    "            name = i['place']['name']\n",
    "            type = i['place']['type']\n",
    "            country = i['place']['country']\n",
    "            countryCode = i['place']['countryCode']\n",
    "            cur.execute('''INSERT OR IGNORE INTO Places (fullName, name, type, country, countryCode)\n",
    "            VALUES ( ?, ? , ? , ?, ?  )''', ( fullName, name, type, country, countryCode) )\n",
    "            cur.execute('SELECT id FROM Places WHERE fullName = ? AND type = ? ', (fullName, type ))\n",
    "            place_id = cur.fetchone()[0]\n",
    "\n",
    "        sourceLabel = i['sourceLabel']\n",
    "        source = i['source']\n",
    "        sourceUrl = i['sourceUrl']\n",
    "\n",
    "        cur.execute('''INSERT OR IGNORE INTO Sources (sourceLabel, source, sourceUrl)\n",
    "        VALUES ( ?, ? , ?  )''', ( sourceLabel, source, sourceUrl) )\n",
    "        cur.execute('SELECT id FROM Sources WHERE sourceLabel = ? ', (sourceLabel, ))\n",
    "        source_id = cur.fetchone()[0]\n",
    "\n",
    "        id = int(i['id'])\n",
    "        date = i['date']\n",
    "        date_formatted = datetime.datetime.strptime(date[0:19], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        unixdate = int(datetime.datetime.timestamp(date_formatted))\n",
    "\n",
    "        content = i['rawContent']\n",
    "        renderedContent = i['renderedContent']\n",
    "        replyCount = int(i['replyCount'])\n",
    "        retweetCount = int(i['retweetCount'])\n",
    "        likeCount = int(i['likeCount'])\n",
    "        quoteCount = int(i['quoteCount'])\n",
    "        lang = i['lang']\n",
    "        url = i['url']\n",
    "        if i['coordinates'] == None:\n",
    "            coordinates_string = None\n",
    "        else:\n",
    "            coordinates = [float(i['coordinates']['latitude']), float(i['coordinates']['longitude'])]\n",
    "            coordinates_string = ', '.join(str(item) for item in coordinates)\n",
    "\n",
    "        hashtags = i['hashtags']\n",
    "        cashtags = i['cashtags']\n",
    "        user_id = int(i['user']['id'])\n",
    "        if i['mentionedUsers'] == None:\n",
    "            mentionedUsers_str = None\n",
    "        else:\n",
    "            mentionedUsers_ids = [u['id'] for u in i['mentionedUsers']]\n",
    "            mentionedUsers_str = ', '.join(str(item) for item in mentionedUsers_ids)\n",
    "\n",
    "        if i['inReplyToTweetId'] == None:\n",
    "            inReplyToTweetId = None\n",
    "        else:\n",
    "            inReplyToTweetId = int(i['inReplyToTweetId'])\n",
    "\n",
    "        if i['quotedTweet'] == None:\n",
    "            quotedTweetId = None\n",
    "        else:\n",
    "            quotedTweetId = int(i['quotedTweet']['id'])\n",
    "\n",
    "        if i['inReplyToUser'] == None:\n",
    "            inReplyToUser_id = None\n",
    "        else:\n",
    "            inReplyToUser_id = int(i['inReplyToUser']['id'])\n",
    "\n",
    "        conversationId = int(i['conversationId'])\n",
    "\n",
    "        if hashtags == None:\n",
    "            hashtags_string = None\n",
    "        else:\n",
    "            hashtag_ids = [] # list of hashtag ids from the Hashtags table\n",
    "            for h in hashtags:\n",
    "                cur.execute('''INSERT OR IGNORE INTO Hashtags (hashtag ) VALUES ( ?  )''', (h, ) )\n",
    "                cur.execute('SELECT id FROM Hashtags WHERE hashtag = ? ', (h, ))\n",
    "                h_id = cur.fetchone()[0]\n",
    "                hashtag_ids.append(h_id)\n",
    "\n",
    "            hashtags_string = ', '.join(str(item) for item in hashtag_ids)\n",
    "\n",
    "        if cashtags == None:\n",
    "            cashtags_string = None\n",
    "        else:\n",
    "            cashtag_ids = [] # list of hashtag ids from the Hashtags table\n",
    "            if cashtags != None:\n",
    "                for c in cashtags:\n",
    "                    cur.execute('''INSERT OR IGNORE INTO Cashtags (cashtag ) VALUES ( ?  )''', ( c, ) )\n",
    "                    cur.execute('SELECT id FROM Cashtags WHERE cashtag = ? ', (c, ))\n",
    "                    c_id = cur.fetchone()[0]\n",
    "                    cashtag_ids.append(c_id)\n",
    "\n",
    "            cashtags_string = ', '.join(str(item) for item in cashtag_ids)\n",
    "\n",
    "        ## User data####\n",
    "        user_id = int(i['user']['id'])\n",
    "        username = i['user']['username']\n",
    "        displayname = i['user']['displayname']\n",
    "        description = i['user']['renderedDescription']\n",
    "        rawDescription = i['user']['rawDescription']\n",
    "        descriptionUrls = i['user']['descriptionLinks']\n",
    "        if descriptionUrls == None:\n",
    "            descriptionUrls_str = None\n",
    "        else:\n",
    "            descriptionUrls_lst = [item['url'] for item in i['user']['descriptionLinks']]\n",
    "            descriptionUrls_str = ', '.join(descriptionUrls_lst)\n",
    "        verified = int(i['user']['verified'])\n",
    "        created = i['user']['created']\n",
    "        created_formatted = datetime.datetime.strptime(created[0:19], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        unix_created = int(datetime.datetime.timestamp(created_formatted))\n",
    "        followersCount = int(i['user']['followersCount'])\n",
    "        following = int(i['user']['friendsCount'])\n",
    "        statusesCount = int(i['user']['statusesCount'])\n",
    "        favouritesCount = int(i['user']['favouritesCount'])\n",
    "        listedCount = int(i['user']['listedCount'])\n",
    "        mediaCount = int(i['user']['mediaCount'])\n",
    "        location = i['user']['location']\n",
    "        protected = int(i['user']['protected'])\n",
    "        if i['user']['link'] != None:\n",
    "            linkUrl = i['user']['link']['text']\n",
    "        else:\n",
    "            linkUrl = None\n",
    "        # linkUrl = i['user']['linkUrl']\n",
    "        profileImageUrl = i['user']['profileImageUrl']\n",
    "        profileBannerUrl = i['user']['profileBannerUrl']\n",
    "        user_url = i['user']['url']\n",
    "\n",
    "        cur.execute('''INSERT OR IGNORE INTO Users (id, username, displayname, description, rawDescription, descriptionUrls, verified, created, followersCount, following, statusesCount, favouritesCount, listedCount, mediaCount, location, protected, linkUrl, profileImageUrl, profileBannerUrl, url, label_id)\n",
    "        VALUES ( ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ? )''', ( user_id, username, displayname, description, rawDescription, descriptionUrls_str, verified, unix_created, followersCount, following, statusesCount, favouritesCount, listedCount, mediaCount, location, protected, linkUrl, profileImageUrl, profileBannerUrl, user_url, label_id) )\n",
    "        # cur.execute('SELECT id FROM Users WHERE id = ? ', (user_id, ))\n",
    "        # user_id = cur.fetchone()[0]\n",
    "\n",
    "        cur.execute('''INSERT OR IGNORE INTO Tweets (id, date, content, renderedContent, replyCount, retweetCount, likeCount, quoteCount, lang, url, coordinates, hashtags, cashtags, user_id, mentionedUsers_ids, inReplyToTweetId, quotedTweetId, inReplyToUser_id, conversationId, source_id, place_id)\n",
    "        VALUES ( ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ? )''', ( id, unixdate, content, renderedContent, replyCount, retweetCount, likeCount, quoteCount, lang, url, coordinates_string, hashtags_string, cashtags_string, user_id, mentionedUsers_str, inReplyToTweetId, quotedTweetId, inReplyToUser_id, conversationId, source_id, place_id ) )\n",
    "\n",
    "        # cur.execute('''INSERT OR IGNORE INTO Tweets (id, date, content, renderedContent, replyCount, retweetCount, likeCount, quoteCount, lang, url, coordinates, hashtags, cashtags, user_id, mentionedUsers_ids, inReplyToTweetId, quotedTweetId, inReplyToUser_id, conversationId, source_id, place_id)\n",
    "        # VALUES ( ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ? )''', ( id, unixdate, content, renderedContent, replyCount, retweetCount, likeCount, quoteCount, lang, url, coordinates_string, hashtags_string, cashtags_string, user_id, mentionedUsers_str, inReplyToTweetId, quotedTweetId, inReplyToUser_id, conversationId, source_id, 1 ) )\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "print('Finished converting json data into SQLite in {:4.4f} '.format(t1 - t0))\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor\n",
    "cur.close()\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uxyARJY46vKO"
   },
   "source": [
    "### The Twitter API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zkPEhJn_Kga0"
   },
   "source": [
    "- Twitter is also known for being an abundant source of publc text data (perhaps even more so than Reddit).\n",
    "- Twitter provides several types of API that can be used to collect anything from tweets to user descriptions to follower networks.\n",
    "    - You can [read all about it here](https://developer.twitter.com/).\n",
    "- For this tutorial, we'll look at using the [standard search API](https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html), which allows us to retreive tweets that contain specific words, phrases, and hashtags.\n",
    "- In the slides, we talked about how to setup a Twitter App and get a API keys.\n",
    "    - You should add your own keys below and then run the code block to set your keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zdINIx1PLunM"
   },
   "outputs": [],
   "source": [
    "twitter_API_key = \"\"\n",
    "twitter_API_secret_key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a8unaxcDL7gT"
   },
   "source": [
    "- Do not share your credentials with anyone!\n",
    "    - You shouldn't hardcode your API keys in code (like above) if you are going to save the file anywhere that is visible to others (like commiting the file to github).\n",
    "        - You can read more about securing your API keys [here](https://developer.twitter.com/en/docs/basics/authentication/guides/securing-keys-and-tokens).\n",
    "     - So, if you plan to save this file in any way, make sure to remove your API keys first.\n",
    "     - If you think your keys have been compromized, you can regenerate them.\n",
    "        - [Apps](https://developer.twitter.com/en/apps) -> Keys and Tokens -> Regenerate\n",
    "- Now, let's see how we can use the [tweepy](https://github.com/tweepy/tweepy) library to collect some tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHqPNZl-6yQU"
   },
   "outputs": [],
   "source": [
    "# create an auth handler object using the api tokens\n",
    "auth = tweepy.AppAuthHandler(twitter_API_key, twitter_API_secret_key)\n",
    "\n",
    "# tweepy automatically takes care of potential rate limiting issues\n",
    "API = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "# let's look for some tweets\n",
    "query = \"#Apple\"\n",
    "\n",
    "# count: 100 is the max allowed value for this parameter\n",
    "#     though we might get fewer than that\n",
    "# tweet_mode: Twitter changed the char limit from 140->280, but didn't want\n",
    "#     to break applications expecting 140, so we have to make sure to ask for this.\n",
    "tweets = API.search(q=query, count=100, tweet_mode=\"extended\")\n",
    "\n",
    "print(\"Collected\",len(tweets),\"tweets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tuZytN78mSqB"
   },
   "source": [
    "- Great, hopefully you got some tweets! Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PYONLu2mmR0L"
   },
   "outputs": [],
   "source": [
    "print(json.dumps(tweets[0]._json, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4EMpra6doeC6"
   },
   "source": [
    "- Here is the text portion of the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FakwVnGUoR-9"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n\\n\".join([tweet.full_text for tweet in tweets]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KEWIjJ3iooyB"
   },
   "source": [
    "- Things are starting to look a bit more like our examples from the noisy text section.\n",
    "- Note: retweets are cut off with ... Retweets have 2 full_text fields, one may get the other one properly addressing it based on Json if needed\n",
    "- To make it even easier to collect tweets from page to page, we can use the tweepy Cursor object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AT4f7mqTw-m_"
   },
   "outputs": [],
   "source": [
    "cursor = tweepy.Cursor(API.search, q=\"#EDHEC\", tweet_mode=\"extended\")\n",
    "\n",
    "# just get 5 tweets\n",
    "# if not given, will (in theory) retrieve as many matching tweets as possible\n",
    "# (standard search only allows search within previous ~1 week)\n",
    "for tweet in cursor.items(5):\n",
    "    print(tweet.full_text)\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Lyt5mwD9AH7"
   },
   "source": [
    "### Putting it together: building your own corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ruX8QSyVqr09"
   },
   "source": [
    "**Exercise 2:** Tweet collection\n",
    "\n",
    "- Let's write a function to collect a larger set of tweets related to a query\n",
    "    - If you want to collect data using multiple queries, you can just call this function multiple times, changing the query each time.\n",
    "    - Store the tweets in the file howerever you like\n",
    "        - You will need to write your own parser for this file later on in the tutorial.\n",
    "    - Store whatever information you like about each tweet, but collect the `full_text` at the very least.\n",
    "    - Make sure to check if `limit` is set, and if it is, only collect `limit` tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45JYj1e-9O9i"
   },
   "outputs": [],
   "source": [
    "def write_tweets_to_file(API, query, output_filename, limit=5):\n",
    "# ------------- Exercise 2 -------------- #\n",
    "    # gather tweets here, then write to output_filename\n",
    "# ---------------- End ------------------ #\n",
    "\n",
    "# quick test\n",
    "query = \"#twitter\"\n",
    "auth = tweepy.AppAuthHandler(twitter_API_key, twitter_API_secret_key)\n",
    "API = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "output_filename = \"test.txt\"\n",
    "write_tweets_to_file(auth, query, output_filename, limit=3)\n",
    "print(\"Wrote this to the file:\",'\\n'+open(output_filename).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6wFhuwcrdz7"
   },
   "source": [
    "- Now, change the `query` string below to whatever you like, and run the code.\n",
    "    - *Make sure your code above is working before you run this! Otherwise, you may run quite a few queries and hit your rate limit, preventing you from testing your code again for ~15 minutes*\n",
    "    - See [this page](https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators.html) under \"standard search operators\" for details on what kinds of things you can place here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjmMnYD5o8_D"
   },
   "outputs": [],
   "source": [
    "query = \"#Apple\"\n",
    "\n",
    "# call the tweet collection function\n",
    "auth = tweepy.AppAuthHandler(twitter_API_key, twitter_API_secret_key)\n",
    "API = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "output_filename = \"mytweets.txt\"\n",
    "write_tweets_to_file(API, query, output_filename, 1000)\n",
    "\n",
    "# zip and download\n",
    "output_zip = output_filename + '.zip'\n",
    "with zipfile.ZipFile(output_zip, 'w') as myzip:\n",
    "    myzip.write(output_filename)\n",
    "files.download(output_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QjgKtTebEM2w"
   },
   "source": [
    "- Note: with some web browsers, the `files.download()` command won't correctly open a dialog window to download the files.\n",
    "    - If this happens, check out the \"Files\" menu on the sidebar\n",
    "        - can be expanded on the left side of this notebook -- click the > button in the top left-corner to unhide the menu.\n",
    "    - You can download your file there (and also upload it when you need it in the next notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "QjgKtTebEM2w"
   },
   "outputs": [],
   "source": [
    "#@title Sample Solution (double-click to view) {display-mode: \"form\"}\n",
    "\n",
    "def write_tweets_to_file(api, query, output_filename, limit=10):\n",
    "    cursor = tweepy.Cursor(API.search, q=query, tweet_mode=\"extended\")\n",
    "    with open(output_filename,'w') as out:\n",
    "        for tweet in cursor.items(limit):\n",
    "            # using tags since tweets may have newlines in them\n",
    "            # you may also want to write other information to this file,\n",
    "            # or even the entire json object.\n",
    "            out.write('<TWEET>' + tweet.full_text + '</TWEET>\\n')\n",
    "            \n",
    "# quick test\n",
    "query = \"#twitter\"\n",
    "auth = tweepy.AppAuthHandler(twitter_API_key, twitter_API_secret_key)\n",
    "API = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "output_filename = \"test2.txt\"\n",
    "write_tweets_to_file(auth, query, output_filename, limit=3)\n",
    "print(\"Wrote this to the file:\",'\\n'+open(output_filename).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FmOf-jZ1F-k"
   },
   "source": [
    "- You should now have your own file(s) containing Twitter data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etUZgi-twrES"
   },
   "source": [
    "- [-> Next: Corpus Level Processing](https://colab.research.google.com/github/gordeli/BIfTA/blob/main/colab/03_Corpus_Level_Processing.ipynb)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "02_Data_Collection.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
